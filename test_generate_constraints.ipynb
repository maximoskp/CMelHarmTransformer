{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harmony_tokenizers_m21 import ChordSymbolTokenizer, PitchClassTokenizer, MelodyPitchTokenizer, MergedMelHarmTokenizer\n",
    "from data_utils import StructGPTMelHarmDataset, GenCollator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import AutoConfig, GPT2LMHeadModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/media/maindisk/maximos/data/hooktheory_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstok = ChordSymbolTokenizer()\n",
    "pctok = PitchClassTokenizer()\n",
    "meltok = MelodyPitchTokenizer()\n",
    "tokenizer = MergedMelHarmTokenizer(meltok, cstok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = StructGPTMelHarmDataset(test_dir, tokenizer, max_length=512, num_bars=64, return_harmonization_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = GenCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer.vocab),\n",
    "    n_positions=512,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    pad_token_id=tokenizer.vocab[tokenizer.pad_token],\n",
    "    bos_token_id=tokenizer.vocab[tokenizer.bos_token],\n",
    "    eos_token_id=tokenizer.vocab[tokenizer.eos_token],\n",
    "    n_embd=512\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_models/gpt/ChordSymbolTokenizer/ChordSymbolTokenizer.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(547, 512)\n",
       "    (wpe): Embedding(512, 512)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-7): 8 x GPT2Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=1536, nx=512)\n",
       "          (c_proj): Conv1D(nf=512, nx=512)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=2048, nx=512)\n",
       "          (c_proj): Conv1D(nf=512, nx=2048)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=547, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_name = 'cuda:0'\n",
    "if device_name == 'cpu':\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(device_name)\n",
    "    else:\n",
    "        print('Selected device not available: ' + device_name)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=device_name, weights_only=True)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 5\n",
    "for b in batch['input_ids']:\n",
    "    melody_tokens = []\n",
    "    real_tokens = []\n",
    "    generated_tokens = []\n",
    "    # find the start harmony token\n",
    "    start_harmony_position = np.where( b == tokenizer.vocab[tokenizer.harmony_tokenizer.start_harmony_token] )[0][0]\n",
    "    real_ids = b\n",
    "    input_ids = b[:(start_harmony_position+1)].to(device)\n",
    "    for i in input_ids:\n",
    "        melody_tokens.append( tokenizer.ids_to_tokens[ int(i) ].replace(' ','x') )\n",
    "\n",
    "    for i in range(start_harmony_position, len(real_ids), 1):\n",
    "        if real_ids[i] != tokenizer.pad_token_id:\n",
    "            real_tokens.append( tokenizer.ids_to_tokens[ int(real_ids[i]) ].replace(' ','x') )\n",
    "    \n",
    "    # Define the bar token ID, eos_token_id, and per-batch sequence constraints\n",
    "    bar_token_id = tokenizer.vocab['<bar>']\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    bars_count = (batch['input_ids'] == bar_token_id).sum(dim=1).reshape(batch['input_ids'].shape[0],-1)\n",
    "    bars_count = bars_count[0]\n",
    "\n",
    "    try:\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids.reshape(1, input_ids.shape[0]),\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            num_beams=num_beams,\n",
    "        )\n",
    "    except:\n",
    "        print('exception: ', input_ids)\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids.reshape(1, input_ids.shape[0]),\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            num_beams=2,\n",
    "        )\n",
    "    for i in range(start_harmony_position, len(outputs[0]), 1):\n",
    "        generated_tokens.append( tokenizer.ids_to_tokens[ int(outputs[0][i]) ].replace(' ','x') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'constraints_mask'])\n",
      "['<s>', '<bar>', 'ts_4x4', 'position_0x00', 'P:64', 'position_0x50', 'P:64', 'position_1x00', 'P:64', 'position_1x25', 'P:64', 'position_1x75', 'P:64', 'position_3x25', 'P:64', 'position_3x50', 'P:64', 'position_3x75', 'P:62', '<bar>', 'position_0x00', 'P:64', 'position_0x50', 'P:64', 'position_0x75', 'P:64', 'position_1x25', 'P:62', 'position_1x50', 'P:61', 'position_1x75', 'P:57', 'position_2x75', 'P:57', 'position_3x00', 'P:64', 'position_3x25', 'P:64', 'position_3x75', 'P:64', '<bar>', 'position_0x00', 'P:62', 'position_1x50', 'P:64', 'position_1x75', 'P:64', 'position_2x25', 'P:62', 'position_3x25', 'P:62', 'position_3x50', 'P:60', 'position_3x75', 'P:57', '<bar>', 'position_0x00', 'P:57', 'position_2x00', '<rest>', 'position_3x00', '<rest>', 'position_3x25', 'P:64', 'position_3x75', 'P:64', '</m>', '<bar>', '<fill>', '<bar>', '<fill>', '<bar>', 'position_0x00', 'D:min', '<fill>', '<bar>', '<fill>', '<h>']\n",
      "melody_end_index:  66\n",
      "['</m>', '<bar>', '<fill>', '<bar>', '<fill>', '<bar>', 'position_0x00', 'D:min', '<fill>', '<bar>', '<fill>', '<h>']\n"
     ]
    }
   ],
   "source": [
    "print(batch.keys())\n",
    "print(melody_tokens)\n",
    "# find where melody ends\n",
    "melody_end_index = melody_tokens.index('</m>')\n",
    "print('melody_end_index: ', melody_end_index)\n",
    "# keep melody and constraints after end index\n",
    "after_melody = melody_tokens[melody_end_index:]\n",
    "print(after_melody)\n",
    "# find how many bars before constraint and keep the constraints\n",
    "bars_count = 0\n",
    "constraint_tokens = []\n",
    "constraint_found = False\n",
    "i = 0\n",
    "while i < len(after_melody):\n",
    "    if after_melody[i] == '<bar>':\n",
    "        bars_count += 1\n",
    "    i += 1\n",
    "    while i < len(after_melody) and '<bar>' != after_melody[i] and '<fill>' != after_melody[i]:\n",
    "        constraint_found = True\n",
    "        constraint_tokens.append(after_melody[i])\n",
    "        i += 1\n",
    "    if constraint_found:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['position_0x00', 'D:min']\n"
     ]
    }
   ],
   "source": [
    "print(bars_count)\n",
    "print(constraint_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<h>', '<bar>', 'position_0x00', 'A:maj', 'position_2x00', 'A:maj', '<bar>', 'position_0x00', 'A:maj', 'position_2x00', 'A:maj', '<bar>', 'position_0x00', 'A:maj', 'position_2x00', 'A:maj', '<bar>', 'position_0x00', 'A:maj', 'position_2x00', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 11, 16]\n",
      "['position_0x00', 'D:min']\n",
      "['<bar>', 'position_0x00', 'A:maj', 'position_2x00', 'A:maj']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def is_sublist_contiguous(q, d):\n",
    "    q_len = len(q)\n",
    "    for i in range(len(d) - q_len + 1):\n",
    "        if d[i:i + q_len] == q:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# get proper bar in generated tokens\n",
    "bar_idxs = [i for i in range(len(generated_tokens)) if generated_tokens[i] == '<bar>']\n",
    "print(bar_idxs)\n",
    "if bars_count - 1 >= len(bar_idxs):\n",
    "    start_index = -1 # not applicable\n",
    "else:\n",
    "    start_index = bar_idxs[bars_count-1]\n",
    "    if bars_count - 1 == len(bar_idxs) - 1:\n",
    "        end_index = len(bar_idxs)\n",
    "    else:\n",
    "        end_index = bar_idxs[bars_count]\n",
    "constraints_area = None\n",
    "if start_index >= 0:\n",
    "    constraints_area = generated_tokens[start_index:end_index]\n",
    "print(constraint_tokens)\n",
    "print(constraints_area)\n",
    "res = False if constraints_area is None else is_sublist_contiguous(constraint_tokens, constraints_area)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
